%!TEX program = xelatex
%!TEX spellcheck = en_GB
\documentclass[final]{article}
\input{../../.library/preamble.tex}
\input{../../.library/style.tex}
\addbibresource{../../.library/bibliography.bib}
\begin{document}
\section{Implementations}
\label{sec:implementations}

The following text lists each of the implementations that were created along with a short discussion of the results.

\subsection{Naive}
The naive implementation was provided for the assignment.
Many details regarding it have been discussed in the previous section.
For the rest of the report, its runtime averaged over a total of five runs will be used as a reference time for calculating speed-ups.
These times are measured as $t_k = \SI{10.137}{\second}$ and $t = \SI{12.318}{\second}$ for kernel time and total time respectively.
Note that this means that a significant part of the execution time is not spent in the computation kernel and the application speed-up is therefore limited to a theoretical maximum of 5.6x.

\subsection{Optimised}
The optimised implementation contains two major improvements:

\begin{enumerate}
    \item The call to \texttt{cv::split} in \texttt{CalWeight} (l. 87), which splits the entire frame into three channels (arrays) for blue, green and red colour values, was completely removed. Having access to the three channels separately could potentially improve caching and memory access performance in \texttt{CalWeight} because of the elements being available in a sequential manner. However, splitting the entire frame in every iteration (for every call to \texttt{CalWeight}) is highly superfluous. Because \texttt{CalWeight} does not use the entire frame either, but just a subset defined by the current rectangle \texttt{rec}, it was decided to remove it altogether and access the frame directly using a call to \texttt{next\_frame.at<cv::Vec3b>(row\_index, col\_index)[k]}.

    \item During \texttt{CalWeight}, the weight for every pixel in the rectangle is multiplied by a value based on an expression containing values from \texttt{target\_model} and \texttt{target\_candidate}. These values in turn depend on the \emph{bin} the current pixel falls in. Since the amount of bins is defined to be 16, there exist a total of 16 unique values resulting from the mentioned expression. The naive implementation, however, calculates a value for every pixel individually, resulting in a total of 58 x 86 x 3 = 14964 evaluations of an expression that contains the 'difficult' operations division and square root. The optimised implementation pre-calculates all 16 unique multiplier values, resulting in a large reduction in computational overhead.
\end{enumerate}

Other, less significant optimisations are
\begin{itemize}
     \item pre-calculating the kernel function, in stead of recalculating it every iteration;
     \item defining loop limits as constants so the compiler might optimise them;
     \item reducing superfluous memory access by avoiding multiple assignments and/or reads of variables and
     \item avoiding the passing of data from function to function when it can be accessed within the \texttt{MeanShift} class.
\end{itemize}

This first step will prove to be the most important in terms of performance gain, resulting in $t_k = \SI{0.708}{\second}$ and $t = \SI{2.798}{\second}$ (which is a kernal speed-up of 14.3x and an application speed-up of 4.4x).

\subsection{NEON}
Starting from the optimised implementation, very little rewriting had to be done to make the algorithm suitable for NEON execution. For this implementation, most loops were simply modified to process 4 (32-bit \texttt{float}) or even 16 (8-bit \texttt{unsigned char}) values in every iteration. The following parts of the program utilize NEON:

\begin{description}
    \item[\texttt{Init\_target\_frame}], where the normalisation of the Epanechnikov kernel is performed for four elements concurrently.

    \item[\texttt{pdf\_representation}], where the calculation of the bin each pixel belongs to, depending on its BGR values, is calculated using NEON. Although this is hardly computationally intensive, being a division of each channel value by the amount of bins, which equals 16 (and therefore bin = value >> 4), the memory access pattern is a prime example of why one would like to use NEON, as it allows for loading the interleaved BGR channels to three separate arrays for further processing, with a single function call. Since each channel is represented by a single byte, 16 pixels of three channels each can be loaded at the same time.
    Note that the subsequent calculation of \texttt{pdf\_model} itself cannot be performed using NEON as its memory access pattern is highly irregular due to the use of bin values as an index.

    \item[\texttt{CalWeightNEON}], where pre-calculating the multiplier for each bin, as previously discussed, is done for four values at once. As NEON only supports calculation of reciprocals and reciprocal square roots, the operation $c = \sqrt{a \mathbin{/} b}$ was rewritten to $c = 1 \mathbin{/} \sqrt{b \cdot 1 \mathbin{/} a}$ to obtain NEON-implementable code. Like in \texttt{pdf\_representation}, calculating weights could not be performed using NEON due to its irregular indexing of the multiplier array.

    \item[\texttt{track}], in which the loop is completely vectorised. The set-on-greater-than and multiply operations in the original implementation were replaced by NEON's greater-than (resulting in a bitmask of all ones/all zeros) and bit select operation, causing significant speedup.
\end{description}

As expected, the various utilisations of NEON to vectorise operations did cause another jump in performance, yielding $t_k = \SI{0.508}{\second}$ and $t = \SI{2.600}{\second}$. Since only a small part of the total program could be vectorized, mostly due to the irregular access patterns inherent to the algorithm, performance increase over the optimised implementation might be slower than expected.


\subsection{DSP}
An implementation on the DSP was also made, mainly as a step-up to an implementation where work is divided between the ARM and the DSP.
Two of the most performance critical functions (pdf\_representation and CalWeight) were ported to the DSP after which an attempt was made at optimizing their performance on the DSP.
In this section, the shared pool memory management and compiler optimizations are discussed.

\subsubsection{Shared memory}


\subsubsection{Compiler options}
For optimizing the compiler, the hand-tuning manual provided by Texas Instruments was used \cite{handtuning}.
Apart from the standard -O3 flag the only other flag that provided a slight (though barely noticeable) speed-up was the -mt flag that allows the compiler to assume that in one function there won't be "pointer-based parameter writes to a memory location that is read by any other pointer-based parameter to the same function" \cite{handtuning}.
This flag was safe to use because it was made sure that the parameters in both pdf\_representation and CalWeight were write or read-only. Flags that were suggested by the hand-tuning manual but were not used were -mh and -mo. The -mh flag could not be used because there was no padding on both sides of the buffers to allow for speculative loads. The -mo flag was tested but seemed to significantly slow down the code and was therefore not used.

In order to find possible low-level optimizations the code was compiled with the –s –mw –mv6400 flags after which the resulting assembly with compiler comments was inspected. The compiler was mostly sophisticated enough to optimize most loops by itself as the number of loop iterations for all loops in the functions was already set at compile time. For clarity the MUST\_ITERATE pragma's were still included in the DSP code wherever applicable. For the same clarity reasons the function parameters in pdf\_representation and CalWeight were supplied with a restrict qualifier even though the -mt compiler flag should already take care of that.

An attempt was also made at optimally utilising the software pipelining functions of the DSP. During inspection of the assembly it was found that most loops were disqualified for software pipelining due to containing function calls or control-code. After some experimentation the loop in CalWeight that was disqualified for containing control code was able to qualify for software pipelining by coalescing the loop as found in \cref{lst:coalesced}. The resulting software pipelining had both a partitioned and unpartitioned of 3 and a loop carried dependency bound of 3. 
\begin{lstlisting}[style=c, caption=Coalesced loop for weight calculation, label=lst:coalesced]
for (xy = 0; xy < RECT_SIZE; xy++) {
    curr_pixel = frame[y*RECT_COLS+x];
    weight[y*RECT_COLS+x] = multipliers[curr_pixel>>4];
    if (++x == RECT_COLS)
    {
        x = 0;
        y = y + 1;
    }
}
\end{lstlisting} 
This change, like the other changes discussed in this section, unfortunately had very little (if any) impact on the performance.

\subsection{Balanced}
Because two processors are present on the Beagleboard in the form of a DSP and the ARM processor the possibility of processing the frames in parallel was investigated.

\subsection{Fixed-point}
To implement fixed-point arithmetic, we first need to know the dynamic range of all the values in the kernels.
As shown in \cref{fig:value-distributions} the values for the weight are mostly in the sub \num{500} range, while a couple of values reach \num{2000}.
The other values sets are more evenly distributed.
The final ranges are shown in \cref{tab:final-ranges}, the kernel values do not line up with the histogram because we only have to store the normalized values from the histogram, the kernel calculation itself can be done using floats as it is only done once.

\begin{figure}[H]
\centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{resources/histogramC}
        \caption{Histogram of weight values distribution.}
    \end{subfigure}%
    ~
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{resources/histogramp}
        \caption{Histogram of pdf model values distribution.}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{resources/histogramE}
        \caption{Histogram of E. kernel values distribution.}
    \end{subfigure}
    \caption{Value distributions}
    \label{fig:value-distributions}
\end{figure}


\begin{table}[H]
    \centering
    \caption{The final ranges}
    \label{tab:final-ranges}
    \begin{tabular}{lll}
        \toprule
        \textbf{Value set} & \textbf{Range Min} & \textbf{Range Max} \\
        \midrule
        Weight      &  \num{-2048}  &  \num{2048}     \\
        PDF model   &  \num{-1/8}   &  \num{1/8}      \\
        E. kernel      &  \num{-1/512} &  \num{1/512}    \\
        \bottomrule
    \end{tabular}
\end{table}

The speed-up that was measured with a fixed-point implementation based on the \texttt{int32\_t} type, and all values using the largest range (\num{2048}) was about \num{18} to \num{18.6} times kernel speed-up.
The problem is getting a faster conversion from the different ranges and a quick division that respects the fixed point nature of the values in the \texttt{int32\_t} datatype.
Instead of doing a normal integer division, although adding a shift to the nominator should give correct results.
To implement fixed-point math properly the project would need parts of boost for the operator overloads to create a good class, so all the errors and other debugging issues go away.
But linking boost just for that is a step too far for this project.
It was decided to drop the fixed point implementation.

After some benchmarking we found that a 4 way f32 mult takes about 9 cycles, a 4 way 32 mult takes about 5 cycles, and 8 way 16 bit mult takes 3 cycles.

\end{document}